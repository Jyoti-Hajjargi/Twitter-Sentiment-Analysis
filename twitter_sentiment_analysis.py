# -*- coding: utf-8 -*-
"""Twitter Sentiment Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/125KXaeiFt1Kz9LgIvoeYO4wFMK3pgZ3-
"""

import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns
import numpy as sns
import re
import pickle
import nltk
from wordcloud import WordCloud
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('twitter_training.csv')
df.head()

"""Renaming the Columns"""

df.columns = ['Id', 'Category', 'Target', 'Tweet']

"""Checking the Column Names"""

df.head()

"""Checking the Dataset's Dimensions"""

df.shape

"""Inspecting the Dataset's Details"""

df.info()

""" Checking for Missing Data"""

df.isnull().sum()

"""Cleaning the Dataset"""

# Drop rows with null values in the 'text' column
df= df.dropna(subset=['Tweet'])

# Reset the index after dropping rows
df = df.reset_index(drop=True)

# Display the updated DataFrame and the new shape
print(df.head())
print(f"Shape of data after dropping null values: {df.shape}")

""" Analyzing the Sentiment Distribution"""

df['Target'].value_counts()

""" Visualizing Sentiment Distribution"""

# Count the occurrences of each sentiment category
sentiment_counts = df["Target"].value_counts().reset_index()
sentiment_counts.columns = ["Sentiment", "Count"]

# Create a bar plot using Plotly
fig = px.bar(sentiment_counts, x="Sentiment", y="Count", color="Sentiment",
             color_discrete_map={"Positive": "green", "Negative": "red", "Irrelevant": "gray"},
             title="Count of Positive, Negative, Neutral and Irrelevant Tweets")

# Show the plot
fig.show()

""" Analyzing Tweet Lengths"""

# Convert text column to string and handle NaN values
df["Tweet"] = df["Tweet"].astype(str)

# Calculate tweet length
df["Length"] = df["Tweet"].apply(len)

# Calculate average tweet length per sentiment
avg_length_by_sentiment = df.groupby('Target')['Length'].mean().reset_index()

# Create a bar chart to show average tweet length by sentiment
fig = px.bar(avg_length_by_sentiment,
             x="Target",
             y="Length",
             color="Target",
             color_discrete_map={"Positive": "green", "Negative": "red", "Irrelevant": "gray", "Neutral": "blue"},
             title="Average Tweet Length by Sentiment",
             labels={"target": "Sentiment", "Length": "Average Tweet Length"})

# Show the plot
fig.show()

"""Visualizing Sentiment through Word Clouds"""

# Filter data based on sentiment categories
positive_tweets = df[df['Target'] == 'Positive']['Tweet']
negative_tweets = df[df['Target'] == 'Negative']['Tweet']
irrelevant_tweets = df[df['Target'] == 'Irrelevant']['Tweet']
neutral_tweets = df[df['Target'] == 'Neutral']['Tweet']

# Combine all tweets into a single string for each sentiment category
positive_text = " ".join(positive_tweets)
negative_text = " ".join(negative_tweets)
irrelevant_text = " ".join(irrelevant_tweets)
neutral_text = " ".join(neutral_tweets)

# Generate word clouds for each category
def generate_wordcloud(text, title):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(title)
    plt.show()

# Create word clouds for each sentiment
generate_wordcloud(positive_text, "Positive Sentiment Word Cloud")
generate_wordcloud(negative_text, "Negative Sentiment Word Cloud")
generate_wordcloud(irrelevant_text, "Irrelevant Sentiment Word Cloud")
generate_wordcloud(neutral_text, "Neutral Sentiment Word Cloud")

"""Removing Unwanted Words â€“ Stopwords"""

# Download the stopwords resource
nltk.download('stopwords')

# Get the list of stopwords for English
stop_words = stopwords.words('english')

# Print the list of stopwords
print(stop_words)

"""Cleaning and Stemming the Text"""

port_stem = PorterStemmer()
# Function to clean and stem text
def clean_text(text):
    if isinstance(text, float):  # Check if the text is NaN (float)
        text = " "  # Assign a space if NaN

    # Clean the text by removing non-alphabetical characters and converting to lowercase
    text = re.sub('[^a-zA-Z]', ' ', text)
    text = text.lower()

    # Split the text into words
    words = text.split()

    # Remove stopwords and apply stemming
    words = [port_stem.stem(word) for word in words if word not in stop_words]

    # Join the words back into a string with spaces
    cleaned_text = ' '.join(words)

    return cleaned_text

# Example usage with a DataFrame (assuming df is your DataFrame with a 'Tweet' column)
df['Filter_Tweet'] = df['Tweet'].apply(clean_text)

"""Splitting the Data"""

#Splitting the data
X = df['Filter_Tweet'].values
Y = df['Target'].values
print(X)
print(Y)

"""Splitting into Training and Testing Sets"""

X_train, X_test, Y_train, Y_test = train_test_split(X,Y , test_size=0.2 , random_state=2)
print(X.shape , X_train.shape , X_test.shape)
# print(X_train)
print(X_test)

"""Converting Text to Numbers"""

# Converting the textual data into numerical data using TfidfVectorizer
vectorizer = TfidfVectorizer()

# Fit and transform the training data
X_train = vectorizer.fit_transform(X_train)

# Transform the test data using the same vectorizer (don't fit again on test data)
X_test = vectorizer.transform(X_test)

# Print the transformed test data
print(X_train)
print(X_test)

"""Training the Model"""

# Training the Machine Learning Model
model = LogisticRegression(max_iter=1000)
model.fit(X_train, Y_train)

""" Evaluating the Model's Performance"""

# Accuracy Score on training data
X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score(Y_train, X_train_prediction)

# Accuracy Score on test data
X_test_prediction = model.predict(X_test)
testing_data_accuracy = accuracy_score(Y_test, X_test_prediction)

# Print the results with clearer labels
print(f"Accuracy Score on Training Data: {training_data_accuracy:.4f}")
print(f"Accuracy Score on Testing Data: {testing_data_accuracy:.4f}")

"""Saving and Using the Trained Model"""

#saving the trained model
filename='trained_model.sav'
pickle.dump(model,open(filename,'wb'))
#using the saved model for future predictions
#loading the saved model
loaded_model = pickle.load(open('trained_model.sav', 'rb'))
# Select a test sample for prediction
X_new = X_test[200]
Y_actual = Y_test[200]
print("Actual Sentiment:", Y_actual)
# Predict using the trained model
prediction = loaded_model.predict(X_new)
print("Predicted Sentiment:", prediction[0])
if prediction[0] == 'Negative':
    print('Negative Tweet')
elif prediction[0] == 'Positive':
    print('Positive Tweet')
else:
    print('Neutral Tweet')